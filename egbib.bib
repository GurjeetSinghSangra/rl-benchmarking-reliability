@misc{DRL01,
	added-at = {2018-10-11T20:16:13.000+0200},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	biburl = {https://www.bibsonomy.org/bibtex/2b5bd5f75948f959eac4a2bf2fce9ef42/analyst},
	description = {[1709.06560] Deep Reinforcement Learning that Matters},
	interhash = {6f4ef32093f8db0b16430338bda8a326},
	intrahash = {b5bd5f75948f959eac4a2bf2fce9ef42},
	keywords = {2017 arxiv deep-learning paper reinforcement-learning},
	note = {cite arxiv:1709.06560Comment: Accepted to the Thirthy-Second AAAI Conference On Artificial  Intelligence (AAAI), 2018},
	timestamp = {2018-10-11T20:16:13.000+0200},
	title = {Deep Reinforcement Learning that Matters},
	url = {http://arxiv.org/abs/1709.06560},
	year = 2017
}

@inproceedings{VaHasselt,
	author = {van Hasselt, Hado and Guez, Arthur and Hessel, Matteo and Mnih, Volodymyr and Silver, David},
	title = {Learning Values across Many Orders of Magnitude},
	year = {2016},
	isbn = {9781510838819},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Most learning algorithms are not invariant to the scale of the signal that is being approximated. We propose to adaptively normalize the targets used in the learning updates. This is important in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	pages = {4294–4302},
	numpages = {9},
	location = {Barcelona, Spain},
	series = {NIPS'16}
}

@inproceedings{BenchmarkingDRL,
	title={Benchmarking Deep Reinforcement Learning Algorithms},
	author={Felix Helfenstein},
	year={2021}
}

@misc{MBRLBenchmarking,
	title={Benchmarking Model-Based Reinforcement Learning},
	author={Tingwu Wang and Xuchan Bao and Ignasi Clavera and Jerrick Hoang and Yeming Wen and Eric Langlois and Shunshi Zhang and Guodong Zhang and Pieter Abbeel and Jimmy Ba},
	year={2020},
	url={https://openreview.net/forum?id=H1lefTEKDS}
}

@article{GoogleMeasure,
	title={Measuring the Reliability of Reinforcement Learning Algorithms},
	author={Stephanie C. Y. Chan and Sam Fishman and John F. Canny and Anoop Korattikara Balan and Sergio Guadarrama},
	journal={ArXiv},
	year={2020},
	volume={abs/1912.05663}
}

@article{PowerAnalysis,
	author = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	year = {2018},
	month = {06},
	pages = {},
	title = {How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments}
}

@inbook{pwranaly1
	publisher = {John Wiley & Sons, Ltd},
	isbn = {9780470979174},
	title = {Front Matter},
	booktitle = {Data Mining and Statistics for Decision Making},
	chapter = {},
	pages = {},
	doi = {https://doi.org/10.1002/9780470979174.fmatter},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470979174.fmatter},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470979174.fmatter},
	year = {},
	abstract = {Summary The prelims comprise: Half-Title Page Wiley Series Page Title Page Copyright Page Dedication Page Table of Contents Preface Foreword Foreword from the French Language Edition List of Trademarks}
}

@article{pwranaly2,
	author = {Yuan, Ke-Hai and Hayashi, Kentaro},
	title = {Bootstrap approach to inference and power analysis based on three test statistics for covariance structure models},
	journal = {British Journal of Mathematical and Statistical Psychology},
	volume = {56},
	number = {1},
	pages = {93-110},
	doi = {https://doi.org/10.1348/000711003321645368},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1348/000711003321645368},
	eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1348/000711003321645368},
	abstract = {We study several aspects of bootstrap inference for covariance structure models based on three test statistics, including Type I error, power and sample-size determination. Specifically, we discuss conditions for a test statistic to achieve a more accurate level of Type I error, both in theory and in practice. Details on power analysis and sample-size determination are given. For data sets with heavy tails, we propose applying a bootstrap methodology to a transformed sample by a downweighting procedure. One of the key conditions for safe bootstrap inference is generally satisfied by the transformed sample but may not be satisfied by the original sample with heavy tails. Several data sets illustrate that, by combining downweighting and bootstrapping, a researcher may find a nearly optimal procedure for evaluating various aspects of covariance structure models. A rule for handling non-convergence problems in bootstrap replications is proposed.},
	year = {2003}
}


@InProceedings{TRPO,
	title = 	 {Trust Region Policy Optimization},
	author = 	 {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
	booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
	pages = 	 {1889--1897},
	year = 	 {2015},
	editor = 	 {Bach, Francis and Blei, David},
	volume = 	 {37},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Lille, France},
	month = 	 {07--09 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
	url = 	 {https://proceedings.mlr.press/v37/schulman15.html},
	abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}

@article{DQN,
	abstract = {We present the first deep learning model to successfully learn control
	policies directly from high-dimensional sensory input using reinforcement
	learning. The model is a convolutional neural network, trained with a variant
	of Q-learning, whose input is raw pixels and whose output is a value function
	estimating future rewards. We apply our method to seven Atari 2600 games from
	the Arcade Learning Environment, with no adjustment of the architecture or
	learning algorithm. We find that it outperforms all previous approaches on six
	of the games and surpasses a human expert on three of them.},
	added-at = {2019-07-12T20:11:01.000+0200},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
	interhash = {78966703f649bae69a08a6a23a4e8879},
	intrahash = {a00ec4c09f5dc9b3f8a1836f4e02bb5d},
	keywords = {},
	note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
	timestamp = {2019-07-12T20:11:01.000+0200},
	title = {Playing Atari with Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1312.5602},
	year = 2013
}

@inproceedings{IQN,
	added-at = {2019-04-03T00:00:00.000+0200},
	author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Rémi},
	biburl = {https://www.bibsonomy.org/bibtex/265c36dd5f08a87cbeb2e05bbad5cd52c/dblp},
	booktitle = {ICML},
	crossref = {conf/icml/2018},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	ee = {http://proceedings.mlr.press/v80/dabney18a.html},
	interhash = {467a43386b790fb5c0ed20aae883e9a5},
	intrahash = {65c36dd5f08a87cbeb2e05bbad5cd52c},
	keywords = {dblp},
	pages = {1104-1113},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	timestamp = {2019-04-04T11:43:21.000+0200},
	title = {Implicit Quantile Networks for Distributional Reinforcement Learning.},
	url = {http://dblp.uni-trier.de/db/conf/icml/icml2018.html#DabneyOSM18},
	volume = 80,
	year = 2018
}

@article{Rainbow,
	abstract = {The deep reinforcement learning community has made several independent
	improvements to the DQN algorithm. However, it is unclear which of these
	extensions are complementary and can be fruitfully combined. This paper
	examines six extensions to the DQN algorithm and empirically studies their
	combination. Our experiments show that the combination provides
	state-of-the-art performance on the Atari 2600 benchmark, both in terms of data
	efficiency and final performance. We also provide results from a detailed
	ablation study that shows the contribution of each component to overall
	performance.},
	added-at = {2020-03-03T23:40:35.000+0100},
	author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	biburl = {https://www.bibsonomy.org/bibtex/223e587e36693531fa1a5bd1f8d2a1bdd/kirk86},
	description = {[1710.02298] Rainbow: Combining Improvements in Deep Reinforcement Learning},
	interhash = {f4fb4d30fac6e6290d70a94e7420777a},
	intrahash = {23e587e36693531fa1a5bd1f8d2a1bdd},
	keywords = {reinforcement-learning},
	note = {cite arxiv:1710.02298Comment: Under review as a conference paper at AAAI 2018},
	timestamp = {2020-03-03T23:40:35.000+0100},
	title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1710.02298},
	year = 2017
}

@inproceedings{C51,
	added-at = {2019-04-03T00:00:00.000+0200},
	author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
	biburl = {https://www.bibsonomy.org/bibtex/24bd7b917ff18804198bdc605690c7bab/dblp},
	booktitle = {ICML},
	crossref = {conf/icml/2017},
	editor = {Precup, Doina and Teh, Yee Whye},
	ee = {http://proceedings.mlr.press/v70/bellemare17a.html},
	interhash = {90c5c324a6b0d5f026ee5c33c3abc193},
	intrahash = {4bd7b917ff18804198bdc605690c7bab},
	keywords = {dblp},
	pages = {449-458},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	timestamp = {2019-05-30T11:53:39.000+0200},
	title = {A Distributional Perspective on Reinforcement Learning.},
	url = {http://dblp.uni-trier.de/db/conf/icml/icml2017.html#BellemareDM17},
	volume = 70,
	year = 2017
}











