@misc{DRL01,
	added-at = {2018-10-11T20:16:13.000+0200},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	biburl = {https://www.bibsonomy.org/bibtex/2b5bd5f75948f959eac4a2bf2fce9ef42/analyst},
	description = {[1709.06560] Deep Reinforcement Learning that Matters},
	interhash = {6f4ef32093f8db0b16430338bda8a326},
	intrahash = {b5bd5f75948f959eac4a2bf2fce9ef42},
	keywords = {2017 arxiv deep-learning paper reinforcement-learning},
	note = {cite arxiv:1709.06560Comment: Accepted to the Thirthy-Second AAAI Conference On Artificial  Intelligence (AAAI), 2018},
	timestamp = {2018-10-11T20:16:13.000+0200},
	title = {Deep Reinforcement Learning that Matters},
	url = {http://arxiv.org/abs/1709.06560},
	year = 2017
}

@inproceedings{VaHasselt,
	author = {van Hasselt, Hado and Guez, Arthur and Hessel, Matteo and Mnih, Volodymyr and Silver, David},
	title = {Learning Values across Many Orders of Magnitude},
	year = {2016},
	isbn = {9781510838819},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Most learning algorithms are not invariant to the scale of the signal that is being approximated. We propose to adaptively normalize the targets used in the learning updates. This is important in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	pages = {4294–4302},
	numpages = {9},
	location = {Barcelona, Spain},
	series = {NIPS'16}
}

@inproceedings{BenchmarkingDRL,
	title={Benchmarking Deep Reinforcement Learning Algorithms},
	author={Felix Helfenstein},
	year={2021}
}

@misc{MBRLBenchmarking,
	title={Benchmarking Model-Based Reinforcement Learning},
	author={Tingwu Wang and Xuchan Bao and Ignasi Clavera and Jerrick Hoang and Yeming Wen and Eric Langlois and Shunshi Zhang and Guodong Zhang and Pieter Abbeel and Jimmy Ba},
	year={2020},
	url={https://openreview.net/forum?id=H1lefTEKDS}
}

@article{GoogleMeasure,
	title={Measuring the Reliability of Reinforcement Learning Algorithms},
	author={Stephanie C. Y. Chan and Sam Fishman and John F. Canny and Anoop Korattikara Balan and Sergio Guadarrama},
	journal={ArXiv},
	year={2020},
	volume={abs/1912.05663}
}

@article{PowerAnalysis,
	author = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	year = {2018},
	month = {06},
	pages = {},
	title = {How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments}
}

@article{pwranaly2,
	author = {Yuan, Ke-Hai and Hayashi, Kentaro},
	title = {Bootstrap approach to inference and power analysis based on three test statistics for covariance structure models},
	journal = {British Journal of Mathematical and Statistical Psychology},
	volume = {56},
	number = {1},
	pages = {93-110},
	doi = {https://doi.org/10.1348/000711003321645368},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1348/000711003321645368},
	eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1348/000711003321645368},
	abstract = {We study several aspects of bootstrap inference for covariance structure models based on three test statistics, including Type I error, power and sample-size determination. Specifically, we discuss conditions for a test statistic to achieve a more accurate level of Type I error, both in theory and in practice. Details on power analysis and sample-size determination are given. For data sets with heavy tails, we propose applying a bootstrap methodology to a transformed sample by a downweighting procedure. One of the key conditions for safe bootstrap inference is generally satisfied by the transformed sample but may not be satisfied by the original sample with heavy tails. Several data sets illustrate that, by combining downweighting and bootstrapping, a researcher may find a nearly optimal procedure for evaluating various aspects of covariance structure models. A rule for handling non-convergence problems in bootstrap replications is proposed.},
	year = {2003}
}


@InProceedings{TRPO,
	title = 	 {Trust Region Policy Optimization},
	author = 	 {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
	booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
	pages = 	 {1889--1897},
	year = 	 {2015},
	editor = 	 {Bach, Francis and Blei, David},
	volume = 	 {37},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Lille, France},
	month = 	 {07--09 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
	url = 	 {https://proceedings.mlr.press/v37/schulman15.html},
	abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}

@article{DQN,
	abstract = {We present the first deep learning model to successfully learn control
	policies directly from high-dimensional sensory input using reinforcement
	learning. The model is a convolutional neural network, trained with a variant
	of Q-learning, whose input is raw pixels and whose output is a value function
	estimating future rewards. We apply our method to seven Atari 2600 games from
	the Arcade Learning Environment, with no adjustment of the architecture or
	learning algorithm. We find that it outperforms all previous approaches on six
	of the games and surpasses a human expert on three of them.},
	added-at = {2019-07-12T20:11:01.000+0200},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
	interhash = {78966703f649bae69a08a6a23a4e8879},
	intrahash = {a00ec4c09f5dc9b3f8a1836f4e02bb5d},
	keywords = {},
	note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
	timestamp = {2019-07-12T20:11:01.000+0200},
	title = {Playing Atari with Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1312.5602},
	year = 2013
}

@inproceedings{IQN,
	added-at = {2019-04-03T00:00:00.000+0200},
	author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Rémi},
	biburl = {https://www.bibsonomy.org/bibtex/265c36dd5f08a87cbeb2e05bbad5cd52c/dblp},
	booktitle = {ICML},
	crossref = {conf/icml/2018},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	ee = {http://proceedings.mlr.press/v80/dabney18a.html},
	interhash = {467a43386b790fb5c0ed20aae883e9a5},
	intrahash = {65c36dd5f08a87cbeb2e05bbad5cd52c},
	keywords = {dblp},
	pages = {1104-1113},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	timestamp = {2019-04-04T11:43:21.000+0200},
	title = {Implicit Quantile Networks for Distributional Reinforcement Learning.},
	url = {http://dblp.uni-trier.de/db/conf/icml/icml2018.html#DabneyOSM18},
	volume = 80,
	year = 2018
}

@article{Rainbow,
	abstract = {The deep reinforcement learning community has made several independent
	improvements to the DQN algorithm. However, it is unclear which of these
	extensions are complementary and can be fruitfully combined. This paper
	examines six extensions to the DQN algorithm and empirically studies their
	combination. Our experiments show that the combination provides
	state-of-the-art performance on the Atari 2600 benchmark, both in terms of data
	efficiency and final performance. We also provide results from a detailed
	ablation study that shows the contribution of each component to overall
	performance.},
	added-at = {2020-03-03T23:40:35.000+0100},
	author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	biburl = {https://www.bibsonomy.org/bibtex/223e587e36693531fa1a5bd1f8d2a1bdd/kirk86},
	description = {[1710.02298] Rainbow: Combining Improvements in Deep Reinforcement Learning},
	interhash = {f4fb4d30fac6e6290d70a94e7420777a},
	intrahash = {23e587e36693531fa1a5bd1f8d2a1bdd},
	keywords = {reinforcement-learning},
	note = {cite arxiv:1710.02298Comment: Under review as a conference paper at AAAI 2018},
	timestamp = {2020-03-03T23:40:35.000+0100},
	title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1710.02298},
	year = 2017
}

@inproceedings{C51,
	added-at = {2019-04-03T00:00:00.000+0200},
	author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
	biburl = {https://www.bibsonomy.org/bibtex/24bd7b917ff18804198bdc605690c7bab/dblp},
	booktitle = {ICML},
	crossref = {conf/icml/2017},
	editor = {Precup, Doina and Teh, Yee Whye},
	ee = {http://proceedings.mlr.press/v70/bellemare17a.html},
	interhash = {90c5c324a6b0d5f026ee5c33c3abc193},
	intrahash = {4bd7b917ff18804198bdc605690c7bab},
	keywords = {dblp},
	pages = {449-458},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	timestamp = {2019-05-30T11:53:39.000+0200},
	title = {A Distributional Perspective on Reinforcement Learning.},
	url = {http://dblp.uni-trier.de/db/conf/icml/icml2017.html#BellemareDM17},
	volume = 70,
	year = 2017
}

@article{Cvar,
	author = {Acerbi, Carlo and Tasche, Dirk},
	title = {Expected Shortfall: A Natural Coherent Alternative to Value at Risk},
	journal = {Economic Notes},
	volume = {31},
	number = {2},
	pages = {379-388},
	doi = {https://doi.org/10.1111/1468-0300.00091},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0300.00091},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0300.00091},
	abstract = {We discuss the coherence properties of expected shortfall (ES) as a financial risk measure. This statistic arises in a natural way from the estimation of the ‘average of the 100\% worst losses’ in a sample of returns to a portfolio. Here p is some fixed confidence level. We also compare several alternative representations of ES which turn out to be more appropriate for certain purposes (J.E.L.: G20, C13, C14).},
	year = {2002}
}

@article{FDR,
	author = {Yoav Benjamini and Daniel Yekutieli},
	title = {{The control of the false discovery rate in multiple testing under dependency}},
	volume = {29},
	journal = {The Annals of Statistics},
	number = {4},
	publisher = {Institute of Mathematical Statistics},
	pages = {1165 -- 1188},
	keywords = {comparisons with control, discrete test statistics, FDR, Hochberg’s procedure, MTP2 densities, Multiple comparisons procedures, multiple endpoints many-to-one comparisons, positive regression dependency, Simes’equality, unidimensional latent variables},
	year = {2001},
	doi = {10.1214/aos/1013699998},
	URL = {https://doi.org/10.1214/aos/1013699998}
}


@InProceedings{SAC,
	title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
	author =       {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {1861--1870},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {10--15 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
	url = 	 {https://proceedings.mlr.press/v80/haarnoja18b.html},
	abstract = 	 {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
}

@article{MBRL1,
	author    = {Thomas M. Moerland and
	Joost Broekens and
	Catholijn M. Jonker},
	title     = {Model-based Reinforcement Learning: {A} Survey},
	journal   = {CoRR},
	volume    = {abs/2006.16712},
	year      = {2020},
	url       = {https://arxiv.org/abs/2006.16712},
	eprinttype = {arXiv},
	eprint    = {2006.16712},
	timestamp = {Thu, 02 Jul 2020 14:42:48 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2006-16712.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DeepMind,
	author    = {Yuval Tassa and
	Yotam Doron and
	Alistair Muldal and
	Tom Erez and
	Yazhe Li and
	Diego de Las Casas and
	David Budden and
	Abbas Abdolmaleki and
	Josh Merel and
	Andrew Lefrancq and
	Timothy P. Lillicrap and
	Martin A. Riedmiller},
	title     = {DeepMind Control Suite},
	journal   = {CoRR},
	volume    = {abs/1801.00690},
	year      = {2018},
	url       = {http://arxiv.org/abs/1801.00690},
	eprinttype = {arXiv},
	eprint    = {1801.00690},
	timestamp = {Mon, 22 Jul 2019 16:19:02 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1801-00690.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{OpenAI,
	author    = {Greg Brockman and
	Vicki Cheung and
	Ludwig Pettersson and
	Jonas Schneider and
	John Schulman and
	Jie Tang and
	Wojciech Zaremba},
	title     = {OpenAI Gym},
	journal   = {CoRR},
	volume    = {abs/1606.01540},
	year      = {2016},
	url       = {http://arxiv.org/abs/1606.01540},
	eprinttype = {arXiv},
	eprint    = {1606.01540},
	timestamp = {Fri, 08 Nov 2019 12:51:06 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/BrockmanCPSSTZ16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Pybullet,
	title={Pybullet, a python module for physics simulation in robotics, games and machine learning},
	author={Coumans, Erwin and Bai, Yunfei},
	year={2017}
}












